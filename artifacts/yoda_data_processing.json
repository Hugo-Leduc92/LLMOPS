{
  "components": {
    "comp-data-transformation-component": {
      "executorLabel": "exec-data-transformation-component",
      "inputDefinitions": {
        "parameters": {
          "raw_dataset_uri": {
            "parameterType": "STRING"
          },
          "train_test_split_ratio": {
            "parameterType": "NUMBER_DOUBLE"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "test_dataset": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "train_dataset": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    }
  },
  "deploymentSpec": {
    "executors": {
      "exec-data-transformation-component": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "data_transformation_component"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'pandas>=2.3.2' 'datasets==4.0.0' 'gcsfs'  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.14.6' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef data_transformation_component(\n    raw_dataset_uri: str,\n    train_test_split_ratio: float,\n    train_dataset: OutputPath(\"Dataset\"),  # type: ignore\n    test_dataset: OutputPath(\"Dataset\"),  # type: ignore\n) -> None:\n    \"\"\"Format and split Yoda Sentences for Phi-3 fine-tuning.\n\n    Structure mirrors Correction's component interface while preserving existing\n    transformation logic (column choices and variant handling with defaults).\n    \"\"\"\n    import logging\n    import json\n    import pandas as pd\n    from datasets import Dataset\n\n    # Defaults preserved from previous implementation\n    user_column = \"sentence\"\n    assistant_column = \"translation\"\n    assistant_fallback_column = \"translation_extra\"\n    prefer_extra = True\n    emit_both_variants = False\n    random_seed = 42\n\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n    logger.info(\"Starting data transformation process...\")\n\n    logger.info(f\"Reading from {raw_dataset_uri}\")\n    df = pd.read_csv(raw_dataset_uri)\n\n    # Validate columns\n    expected = {user_column}\n    if assistant_column:\n        expected.add(assistant_column)\n    if assistant_fallback_column:\n        expected.add(assistant_fallback_column)\n    missing = [c for c in expected if c not in df.columns]\n    if missing:\n        raise ValueError(f\"Missing expected columns: {missing}\")\n\n    # Build conversations using selected assistant column\n    conversations = []\n    for _, row in df.iterrows():\n        user_text = str(row[user_column])\n        primary_ok = assistant_column in df.columns and pd.notna(row[assistant_column])\n        extra_ok = assistant_fallback_column in df.columns and pd.notna(row[assistant_fallback_column])\n\n        if prefer_extra and extra_ok:\n            assistant_text = str(row[assistant_fallback_column])\n        elif primary_ok:\n            assistant_text = str(row[assistant_column])\n        elif extra_ok:\n            assistant_text = str(row[assistant_fallback_column])\n        else:\n            assistant_text = \"\"\n\n        conversations.append([\n            {\"role\": \"user\", \"content\": user_text},\n            {\"role\": \"assistant\", \"content\": assistant_text},\n        ])\n\n        if emit_both_variants and primary_ok and extra_ok:\n            other_text = str(row[assistant_column]) if prefer_extra else str(row[assistant_fallback_column])\n            conversations.append([\n                {\"role\": \"user\", \"content\": user_text},\n                {\"role\": \"assistant\", \"content\": other_text},\n            ])\n\n    ds = Dataset.from_dict({\"messages\": [json.dumps(conv) for conv in conversations]})\n    logger.info(\"Splitting dataset: test_size=%.2f seed=%d\", train_test_split_ratio, random_seed)\n    split = ds.train_test_split(test_size=train_test_split_ratio, seed=random_seed)\n\n    logger.info(f\"Writing train dataset to {train_dataset}...\")\n    split[\"train\"].to_csv(train_dataset, index=False)\n\n    logger.info(f\"Writing test dataset to {test_dataset}...\")\n    split[\"test\"].to_csv(test_dataset, index=False)\n\n    logger.info(\"Data transformation process completed successfully\")\n\n"
          ],
          "image": "python:3.11-slim"
        }
      }
    }
  },
  "pipelineInfo": {
    "name": "yoda-data-processing-pipeline"
  },
  "root": {
    "dag": {
      "tasks": {
        "data-transformation-component": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-data-transformation-component"
          },
          "inputs": {
            "parameters": {
              "raw_dataset_uri": {
                "componentInputParameter": "raw_dataset_uri"
              },
              "train_test_split_ratio": {
                "runtimeValue": {
                  "constant": 0.2
                }
              }
            }
          },
          "taskInfo": {
            "name": "data-transformation-component"
          }
        }
      }
    },
    "inputDefinitions": {
      "parameters": {
        "raw_dataset_uri": {
          "parameterType": "STRING"
        }
      }
    }
  },
  "schemaVersion": "2.1.0",
  "sdkVersion": "kfp-2.14.6"
}